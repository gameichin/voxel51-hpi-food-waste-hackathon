{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb53e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb1750b",
   "metadata": {},
   "source": [
    "## Library Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets library\n",
    "import fiftyone as fo\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3702d2",
   "metadata": {},
   "source": [
    "### Load the datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e36d5a",
   "metadata": {},
   "source": [
    "- Old Ds is the first dataset\n",
    "- Hf Ds is the hugginf face dataset, which we turned into a fiftyone dataset\n",
    "- We transformed the german columns to English columns, as well as renamed the ingredients to english, as shown in the demo notebook\n",
    "- We cleaned the hf ds, before combining it with the old dataset\n",
    "- All the 3 datasets are loaded below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5415c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ds = fo.Dataset.from_dir(\n",
    "    dataset_dir=\"../food_waste_part_1\", dataset_type=fo.types.FiftyOneDataset\n",
    ")\n",
    "hf_ds = fo.Dataset.from_dir(\n",
    "    dataset_dir=\"../food_waste_part_2\", dataset_type=fo.types.FiftyOneDataset\n",
    ")\n",
    "hf_old_ds = fo.Dataset.from_dir(\n",
    "    dataset_dir=\"../old_ds_hf_combined\", dataset_type=fo.types.FiftyOneDataset\n",
    ")\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3c0c7",
   "metadata": {},
   "source": [
    "### Create FO from collected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714677f4",
   "metadata": {},
   "source": [
    "- We created a fiftyone dataset from the collected data\n",
    "- The process to create the necessary fields was done by parsing. \n",
    "    - First we took the folder names and used them as the file names. \n",
    "    - We later parsed these file names to fill the ingredient names and return quantity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def rename_files_with_folder_name(folder_path):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: '{folder_path}' is not a valid directory.\")\n",
    "        return\n",
    "\n",
    "    # Get the folder name\n",
    "    folder_name = os.path.basename(os.path.normpath(folder_path))\n",
    "\n",
    "    # Get a list of all files in the folder\n",
    "    files = [\n",
    "        f\n",
    "        for f in os.listdir(folder_path)\n",
    "        if os.path.isfile(os.path.join(folder_path, f))\n",
    "    ]\n",
    "\n",
    "    # Sort the files to ensure consistent numbering\n",
    "    files.sort()\n",
    "\n",
    "    for i, old_file_name in enumerate(files, 1):\n",
    "        # Get the file extension\n",
    "        _, file_extension = os.path.splitext(old_file_name)\n",
    "\n",
    "        # Create the new file name\n",
    "        new_file_name = f\"{folder_name}_{i}{file_extension}\"\n",
    "\n",
    "        # Construct the full paths\n",
    "        old_file_path = os.path.join(folder_path, old_file_name)\n",
    "        new_file_path = os.path.join(folder_path, new_file_name)\n",
    "\n",
    "        # Rename the file\n",
    "        try:\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            # print(f\"Renamed '{old_file_name}' to '{new_file_name}'\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error renaming file {old_file_name}: {e}\")\n",
    "\n",
    "\n",
    "folder_list = [\n",
    "    \"../group_1/goulash_0-rice_36-potatoes_0\",\n",
    "    \"../group_1/goulash_0-rice_36-potatoes_0-chickpeas_62\",\n",
    "    \"../group_1/goulash_0-rice_36-potatoes_26-chickpeas_62\",\n",
    "    \"../group_4/Goulash_129\",\n",
    "    \"../group_4/Rice_48\",\n",
    "    \"../group_4/Potatoes_101\",\n",
    "    \"../group_4/Rice_48-Potatoes_101-Goulash_129\",\n",
    "    \"../group_3/goulash_25\",\n",
    "    \"../group_3/goulash_25-rice_31-potatoes_21\",\n",
    "    \"../group_3/goulash_92\",\n",
    "    \"../group_3/goulash_92-rice_66-potatoes_103\",\n",
    "    \"../group_3/potatoes_21\",\n",
    "    \"../group_3/potatoes_103\",\n",
    "    \"../group_3/rice_31\",\n",
    "    \"../group_3/rice_66\",\n",
    "]\n",
    "for folder in folder_list:\n",
    "    rename_files_with_folder_name(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import os\n",
    "\n",
    "\n",
    "# --- Define the filename parsing logic --- #\n",
    "def parse_filename(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    name, _ = os.path.splitext(filename)\n",
    "\n",
    "    parts = name.split(\"-\")\n",
    "\n",
    "    ingredient_names = []\n",
    "    quantities = []\n",
    "\n",
    "    for part in parts:\n",
    "        tokens = part.split(\"_\")\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "\n",
    "        ing_name = tokens[0].lower()  # ensure lowercase\n",
    "        try:\n",
    "            qty = float(tokens[1])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        ingredient_names.append(ing_name)\n",
    "        quantities.append(qty)\n",
    "\n",
    "    return ingredient_names, quantities\n",
    "\n",
    "\n",
    "# --- Create the dataset --- #\n",
    "dataset = fo.Dataset(\"kool\")\n",
    "\n",
    "# Replace with your actual image directory path\n",
    "image_dir = \"../new_data\"\n",
    "dataset.add_images_dir(image_dir, recursive=True)\n",
    "\n",
    "# --- Add custom fields --- #\n",
    "dataset.add_sample_field(\"ingredient_name\", fo.ListField, subfield=fo.StringField)\n",
    "dataset.add_sample_field(\"return_quantity\", fo.ListField, subfield=fo.FloatField)\n",
    "\n",
    "# --- Populate fields --- #\n",
    "for sample in dataset:\n",
    "    ingredient_names, quantities = parse_filename(sample.filepath)\n",
    "    sample[\"ingredient_name\"] = ingredient_names\n",
    "    sample[\"return_quantity\"] = quantities\n",
    "    sample.save()\n",
    "\n",
    "# --- Show dataset summary --- #\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94225c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.export(\"../new_data_ds\", fo.types.FiftyOneDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_ds = fo.Dataset.from_dir(\n",
    "    dataset_dir=\"../new_data_ds\", dataset_type=fo.types.FiftyOneDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e76ad",
   "metadata": {},
   "source": [
    "### Combine the old_and_hf_ds with new data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61f436",
   "metadata": {},
   "source": [
    "- Lastly before doing any training we combined the old_and_hf_ds with new collected data\n",
    "- We exported this combined data as a fiftyone dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_resolve_conflicts_fixed(dataset1, dataset2, target_name):\n",
    "    \"\"\"Safely resolve conflicts with proper schema handling\"\"\"\n",
    "\n",
    "    # Delete existing dataset if it exists\n",
    "    try:\n",
    "        existing = fo.load_dataset(target_name)\n",
    "        existing.delete()\n",
    "        print(f\"Deleted existing dataset: {target_name}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Create new unified dataset\n",
    "    unified = fo.Dataset(target_name)\n",
    "\n",
    "    # Define field type mappings\n",
    "    listfield_conversions = {\n",
    "        # Nutritional data should be floats\n",
    "        \"kcal_per_plate\": float,\n",
    "        \"kj_per_plate\": float,\n",
    "        \"fat_per_plate\": float,\n",
    "        \"saturated_fat_per_plate\": float,\n",
    "        \"carbohydrates_per_plate\": float,\n",
    "        \"sugar_per_plate\": float,\n",
    "        \"protein_per_plate\": float,\n",
    "        \"salt_per_plate\": float,\n",
    "        \"weight_per_portion\": float,\n",
    "        \"weight_per_plate\": float,\n",
    "        \"return_quantity\": float,\n",
    "        \"return_percentage\": float,\n",
    "        # Portions should be integers\n",
    "        \"number_of_portions\": int,\n",
    "        # IDs and names should be strings\n",
    "        \"bon_id\": str,\n",
    "        \"article_number\": str,\n",
    "        \"ingredient_name\": str,\n",
    "        \"piece_article\": str,\n",
    "    }\n",
    "\n",
    "    single_field_conversions = {\n",
    "        # Before measurements (floats)\n",
    "        \"salt_before\": float,\n",
    "        \"carbohydrates_before\": float,\n",
    "        \"kcal_before\": float,\n",
    "        \"saturated_fat_before\": float,\n",
    "        \"kj_before\": float,\n",
    "        \"fat_before\": float,\n",
    "        \"sugar_before\": float,\n",
    "        \"protein_before\": float,\n",
    "        # After measurements (integers)\n",
    "        \"saturated_fat_after\": int,\n",
    "        \"fat_after\": int,\n",
    "        \"kcal_after\": int,\n",
    "        \"kj_after\": int,\n",
    "        \"protein_after\": int,\n",
    "        \"sugar_after\": int,\n",
    "        \"salt_after\": int,\n",
    "        \"carbohydrates_after\": int,\n",
    "    }\n",
    "\n",
    "    def convert_value(value, target_type):\n",
    "        \"\"\"Convert a single value to target type, handling None properly\"\"\"\n",
    "        if value is None or value == \"\" or value == \"None\":\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            if target_type == float:\n",
    "                return float(value)\n",
    "            elif target_type == int:\n",
    "                return int(float(value))\n",
    "            elif target_type == str:\n",
    "                return str(value)\n",
    "            else:\n",
    "                return value\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def convert_list_values(value_list, target_type):\n",
    "        \"\"\"Convert all values in a list to target type, filtering out None values\"\"\"\n",
    "        if value_list is None:\n",
    "            return None\n",
    "\n",
    "        if not isinstance(value_list, list):\n",
    "            # If it's not a list, try to convert the single value and make it a list\n",
    "            converted = convert_value(value_list, target_type)\n",
    "            return [converted] if converted is not None else []\n",
    "\n",
    "        converted_list = []\n",
    "        for item in value_list:\n",
    "            converted = convert_value(item, target_type)\n",
    "            if converted is not None:  # Only add non-None values\n",
    "                converted_list.append(converted)\n",
    "\n",
    "        # Return None if list is empty, otherwise return the converted list\n",
    "        return converted_list if converted_list else None\n",
    "\n",
    "    def create_clean_sample(original_sample):\n",
    "        \"\"\"Create a new sample with proper type conversions\"\"\"\n",
    "\n",
    "        new_sample = fo.Sample(filepath=original_sample.filepath)\n",
    "\n",
    "        for field_name in original_sample.field_names:\n",
    "            if field_name in [\"id\", \"filepath\"]:\n",
    "                continue\n",
    "\n",
    "            value = original_sample[field_name]\n",
    "\n",
    "            # Skip None values entirely to avoid schema inference issues\n",
    "            if value is None:\n",
    "                continue\n",
    "\n",
    "            # Handle ListField conversions\n",
    "            if field_name in listfield_conversions:\n",
    "                target_type = listfield_conversions[field_name]\n",
    "                converted_value = convert_list_values(value, target_type)\n",
    "\n",
    "                # Only set the field if we have valid data\n",
    "                if converted_value is not None and len(converted_value) > 0:\n",
    "                    new_sample[field_name] = converted_value\n",
    "\n",
    "            # Handle single field conversions\n",
    "            elif field_name in single_field_conversions:\n",
    "                target_type = single_field_conversions[field_name]\n",
    "                converted_value = convert_value(value, target_type)\n",
    "\n",
    "                # Only set the field if we have valid data\n",
    "                if converted_value is not None:\n",
    "                    new_sample[field_name] = converted_value\n",
    "\n",
    "            # Handle other fields (copy as-is if not None)\n",
    "            else:\n",
    "                try:\n",
    "                    new_sample[field_name] = value\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not set field {field_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return new_sample\n",
    "\n",
    "    # Process Dataset2 first (it has cleaner schema)\n",
    "    print(\"Processing Dataset2 samples first...\")\n",
    "    dataset2_samples = []\n",
    "    for sample in dataset2.iter_samples(progress=True):\n",
    "        try:\n",
    "            clean_sample = create_clean_sample(sample)\n",
    "            dataset2_samples.append(clean_sample)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Dataset2 sample {sample.id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Add Dataset2 samples first to establish schema\n",
    "    print(\"Adding Dataset2 samples to establish schema...\")\n",
    "    if dataset2_samples:\n",
    "        unified.add_samples(dataset2_samples[:10])  # Add first 10 to establish schema\n",
    "\n",
    "        # Add remaining Dataset2 samples\n",
    "        if len(dataset2_samples) > 10:\n",
    "            remaining_samples = dataset2_samples[10:]\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(remaining_samples), batch_size):\n",
    "                batch = remaining_samples[i : i + batch_size]\n",
    "                unified.add_samples(batch)\n",
    "\n",
    "    # Now process Dataset1 samples\n",
    "    print(\"Processing Dataset1 samples...\")\n",
    "    dataset1_samples = []\n",
    "    for sample in dataset1.iter_samples(progress=True):\n",
    "        try:\n",
    "            clean_sample = create_clean_sample(sample)\n",
    "            dataset1_samples.append(clean_sample)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Dataset1 sample {sample.id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Add Dataset1 samples in batches\n",
    "    print(\"Adding Dataset1 samples...\")\n",
    "    if dataset1_samples:\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(dataset1_samples), batch_size):\n",
    "            batch = dataset1_samples[i : i + batch_size]\n",
    "            try:\n",
    "                unified.add_samples(batch)\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding batch {i // batch_size + 1}: {e}\")\n",
    "                # Try adding samples one by one for this batch\n",
    "                for j, sample in enumerate(batch):\n",
    "                    try:\n",
    "                        unified.add_sample(sample)\n",
    "                    except Exception as sample_error:\n",
    "                        print(f\"  Failed to add sample {i + j}: {sample_error}\")\n",
    "                        continue\n",
    "\n",
    "    unified.persistent = True\n",
    "\n",
    "    print(f\"Successfully created unified dataset with {len(unified)} samples\")\n",
    "\n",
    "    # Print final schema for verification\n",
    "    print(\"\\nFinal schema for key fields:\")\n",
    "    schema = unified.get_field_schema()\n",
    "    key_fields = [\"kcal_per_plate\", \"kj_per_plate\", \"kcal_before\", \"kcal_after\"]\n",
    "    for field_name in key_fields:\n",
    "        if field_name in schema:\n",
    "            field_type = type(schema[field_name]).__name__\n",
    "            print(f\"  {field_name}: {field_type}\")\n",
    "\n",
    "    return unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b176e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "unified_dataset = safe_resolve_conflicts_fixed(\n",
    "    hf_old_ds, collected_ds, \"food_waste_all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fa2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_dataset.export('all_data', fo.types.FiftyOneDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7750dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fo.launch_app(unified_dataset, port=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
