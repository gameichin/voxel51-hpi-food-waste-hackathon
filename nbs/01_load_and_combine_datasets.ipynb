{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install fiftyone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Library Imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6CgoEIrFI_u"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import fiftyone as fo, types\n",
        "import fiftyone.core.fields as fof\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viLR5sNSFJ2y"
      },
      "source": [
        "# Load the food_waste_part_1 FO DS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mkpxRXaE-Ss"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49u8dfyOE_xM"
      },
      "outputs": [],
      "source": [
        "general_path = Path(\"drive/MyDrive/food_waste_part_1/\")\n",
        "print(f\"All folders: {os.listdir(general_path)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFa0CfVdJv9h"
      },
      "outputs": [],
      "source": [
        "fo.list_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnH4-i_2JyYj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a dataset from a directory of images\n",
        "dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=general_path,\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    name=\"food_waste_part_1\"\n",
        ")\n",
        "\n",
        "print(f\"Created dataset with {len(dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xjmc48XKgfN"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset)\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxdZvd_LQNFW"
      },
      "source": [
        "# Transform Food Waste Part 2 Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Load the 2nd ds from hf\n",
        "- Do transformations on the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma-hUzYoLo0G"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"Dldermann/food-waste-dataset-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7U_HXS8PL4b"
      },
      "outputs": [],
      "source": [
        "# Apply your feature mapping\n",
        "feature_mapping = {\n",
        "    'bonid': 'bonid',\n",
        "    'image': 'image',\n",
        "    'Bon_ID': 'bon_id',\n",
        "    'Artikelnummer': 'article_number',\n",
        "    'Artikel': 'ingredient_name',\n",
        "    'Stückartikel': 'piece_article',\n",
        "    'Anzahl_Kellen': 'number_of_portions',\n",
        "    'Gewicht_Kelle': 'weight_per_portion',\n",
        "    'Gewicht_Teller': 'weight_per_plate',\n",
        "    'kcal_Teller': 'kcal_per_plate',\n",
        "    'kj_Teller': 'kj_per_plate',\n",
        "    'Fett_Teller': 'fat_per_plate',\n",
        "    'ges_Fettsäuren_Teller': 'saturated_fat_per_plate',\n",
        "    'Kohlenhydrate_Teller': 'carbohydrates_per_plate',\n",
        "    'Zucker_Teller': 'sugar_per_plate',\n",
        "    'Eiweiß_Teller': 'protein_per_plate',\n",
        "    'Salz_Teller': 'salt_per_plate',\n",
        "    'Menge_Rückläufer': 'return_quantity',\n",
        "    'Prozent_Rückläufer': 'return_percentage',\n",
        "    'Gericht': 'dish',\n",
        "    'Portionsgröße': 'portion_size',\n",
        "    'Gewicht_vorher': 'weight_before',\n",
        "    'kcal_vorher': 'kcal_before',\n",
        "    'kj_vorher': 'kj_before',\n",
        "    'Fett_vorher': 'fat_before',\n",
        "    'ges_Fettsäuren_vorher': 'saturated_fat_before',\n",
        "    'Kohlenhydrate_vorher': 'carbohydrates_before',\n",
        "    'Zucker_vorher': 'sugar_before',\n",
        "    'Eiweiß_vorher': 'protein_before',\n",
        "    'Salz_vorher': 'salt_before',\n",
        "    'Gewicht_nachher': 'weight_after',\n",
        "    'kcal_nachher': 'kcal_after',\n",
        "    'kj_nachher': 'kj_after',\n",
        "    'Fett_nachher': 'fat_after',\n",
        "    'ges_fettsäuren_nachher': 'saturated_fat_after',\n",
        "    'Kohlenhydrate_nachher': 'carbohydrates_after',\n",
        "    'Zucker_nachher': 'sugar_after',\n",
        "    'Eiweiß_nachher': 'protein_after',\n",
        "    'Salz_nachher': 'salt_after'\n",
        "}\n",
        "\n",
        "# German to English ingredient mapping\n",
        "german_to_english_ingredients_hyphenated = {\n",
        "    'Fleischbällchen gebrüht': 'poached-meatballs',\n",
        "    'Reis': 'rice',\n",
        "    'Paniertes Fischfilet': 'breaded-fish-fillet',\n",
        "    'Linseneintopf': 'lentil-stew',\n",
        "    'Apfelmus': 'applesauce',\n",
        "    'Helle Sauce': 'light-sauce-or-white-sauce',\n",
        "    'Kartoffelpüree': 'mashed-potatoes',\n",
        "    'Rinderbraten': 'roast-beef',\n",
        "    'Semmelknödel': 'bread-dumplings',\n",
        "    'Grüne Bohnen': 'green-beans',\n",
        "    'Möhre': 'carrot',\n",
        "    'Pflanzencreme': 'vegetable-based-cream',\n",
        "    'Schinken Mettwurst': 'ham-sausage',\n",
        "    'Paprika': 'paprika-or-bell-pepper',\n",
        "    'Seelachs': 'pollock-or-coalfish',\n",
        "    'Bratenjus': 'gravy',\n",
        "    'Hähnchenstreifen': 'chicken-strips',\n",
        "    'Eisbergsalat': 'iceberg-lettuce',\n",
        "    'Rotkohl': 'red-cabbage',\n",
        "    'Sauerkraut': 'sauerkraut',\n",
        "    'Reibekuchen': 'potato-pancakes-or-potato-fritters',\n",
        "    'Krautsalat': 'coleslaw',\n",
        "    'Schnitzel': 'schnitzel-or-cutlet',\n",
        "    'Blumenkohl': 'cauliflower',\n",
        "    'Rostbratwurst': 'grilled-sausage',\n",
        "    'Braune Sauce': 'brown-sauce',\n",
        "    'Kartoffeln': 'potatoes',\n",
        "    'Kartoffelwürfel': 'diced-potatoes',\n",
        "    'Sahne': 'cream',\n",
        "    'Zucchini': 'zucchini-or-courgette',\n",
        "    'Eierspätzle': 'egg-spaetzle',\n",
        "    'Pilze': 'mushrooms',\n",
        "    'Erbsen': 'peas',\n",
        "    'Wirsing': 'savoy-cabbage',\n",
        "    'Malzbier-Senf-Sauce': 'malt-beer-mustard-sauce',\n",
        "    'Dressing Portion': 'dressing-portion',\n",
        "    'Linsen': 'lentils',\n",
        "    'Zwiebel': 'onion',\n",
        "    'Schweinenackenbraten': 'pork-neck-roast',\n",
        "    'Hähnchen': 'chicken',\n",
        "    'Tomaten-Curry-Sauce': 'tomato-curry-sauce'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnW4KVsbUeRS"
      },
      "outputs": [],
      "source": [
        "hf_dataset = ds.rename_columns(feature_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skkBCyBLU4SV"
      },
      "outputs": [],
      "source": [
        "# Create a persistent directory for images\n",
        "persistent_dir = \"food_waste_dataset-2\"\n",
        "os.makedirs(persistent_dir, exist_ok=True)\n",
        "\n",
        "# Create FiftyOne dataset\n",
        "fiftyone_dataset = fo.load_dataset(name=\"food_waste_dataset-2\")\n",
        "\n",
        "sample_count = 0\n",
        "for split in ['train']:\n",
        "    split_count = 0\n",
        "    for item in hf_dataset[split]:\n",
        "        # Save image to persistent directory with unique filename\n",
        "        image_filename = f\"{split}_{split_count:06d}.jpg\"\n",
        "        image_path = os.path.join(persistent_dir, image_filename)\n",
        "\n",
        "        # Save the PIL Image to the persistent file\n",
        "        item['image'].save(image_path)\n",
        "\n",
        "        # Create FiftyOne sample\n",
        "        sample = fo.Sample(filepath=image_path)\n",
        "        sample['split'] = split\n",
        "\n",
        "        # Add any additional metadata from the original dataset\n",
        "        for key, value in item.items():\n",
        "            if key != 'image':  # Skip the image field since we've handled it\n",
        "                sample[key] = value\n",
        "\n",
        "        fiftyone_dataset.add_sample(sample)\n",
        "        split_count += 1\n",
        "        sample_count += 1\n",
        "\n",
        "print(f\"FiftyOne dataset created with {len(fiftyone_dataset)} samples.\")\n",
        "print(f\"Images saved to: {os.path.abspath(persistent_dir)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2AMQeFlVRA5"
      },
      "outputs": [],
      "source": [
        "fiftyone_dataset.compute_metadata()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GmrMrOMRxI5"
      },
      "outputs": [],
      "source": [
        "session1 = fo.launch_app(fiftyone_dataset)\n",
        "print(session1.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5evjgV2f7nf"
      },
      "outputs": [],
      "source": [
        "fo.list_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flva6GEHfgpT"
      },
      "outputs": [],
      "source": [
        "dataset1 = fo.load_dataset('food_waste_part_1')\n",
        "dataset2 = fo.load_dataset('food_waste_dataset')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEZEUKtV0DhU"
      },
      "source": [
        "# Combine Food Waste Part 2 Dataset with Part 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqCGP5-ckAN_"
      },
      "outputs": [],
      "source": [
        "dataset1 = fo.load_dataset('food_waste_part_1')\n",
        "dataset2 = fo.load_dataset('food_waste_dataset')\n",
        "\n",
        "def diagnose_schema_conflicts(dataset1, dataset2):\n",
        "    \"\"\"Find exact field conflicts between datasets\"\"\"\n",
        "    schema1 = dataset1.get_field_schema(ftype=None, embedded_doc_type=None)\n",
        "    schema2 = dataset2.get_field_schema(ftype=None, embedded_doc_type=None)\n",
        "\n",
        "    conflicts = {}\n",
        "    all_fields = set(schema1.keys()) | set(schema2.keys())\n",
        "\n",
        "    for field_name in all_fields:\n",
        "        field1 = schema1.get(field_name)\n",
        "        field2 = schema2.get(field_name)\n",
        "\n",
        "        if field1 and field2:\n",
        "            type1 = type(field1).__name__\n",
        "            type2 = type(field2).__name__\n",
        "            if type1 != type2:\n",
        "                conflicts[field_name] = (type1, type2)\n",
        "        elif field1:\n",
        "            conflicts[field_name] = (type(field1).__name__, \"Missing\")\n",
        "        elif field2:\n",
        "            conflicts[field_name] = (\"Missing\", type(field2).__name__)\n",
        "\n",
        "    return conflicts\n",
        "\n",
        "conflicts = diagnose_schema_conflicts(dataset1, dataset2)\n",
        "print(\"Schema conflicts:\")\n",
        "for field, (type1, type2) in conflicts.items():\n",
        "    print(f\"  {field}: {type1} vs {type2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCNmvnly0Mo4"
      },
      "source": [
        "# Identify differences between nested fields of the same types (ex. List[str], List[int])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQg3rb9ToBWn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def diagnose_detailed_schema_conflicts(dataset1, dataset2):\n",
        "    \"\"\"Find exact field conflicts including ListField element types and nested fields\"\"\"\n",
        "    schema1 = dataset1.get_field_schema(ftype=None, embedded_doc_type=None)\n",
        "    schema2 = dataset2.get_field_schema(ftype=None, embedded_doc_type=None)\n",
        "\n",
        "    conflicts = {}\n",
        "    all_fields = set(schema1.keys()) | set(schema2.keys())\n",
        "\n",
        "    def get_detailed_field_info(field):\n",
        "        \"\"\"Get detailed information about a field including nested types\"\"\"\n",
        "        if field is None:\n",
        "            return \"Missing\"\n",
        "\n",
        "        field_type = type(field).__name__\n",
        "\n",
        "        # Check ListField element types\n",
        "        if isinstance(field, fof.ListField):\n",
        "            if hasattr(field, 'field') and field.field is not None:\n",
        "                element_type = type(field.field).__name__\n",
        "                return f\"{field_type}({element_type})\"\n",
        "            else:\n",
        "                return f\"{field_type}(Unknown)\"\n",
        "\n",
        "        # Check VectorField dimensions\n",
        "        elif isinstance(field, fof.VectorField):\n",
        "            if hasattr(field, 'dim') and field.dim is not None:\n",
        "                return f\"{field_type}(dim={field.dim})\"\n",
        "            else:\n",
        "                return f\"{field_type}(dim=Unknown)\"\n",
        "\n",
        "        # Check EmbeddedDocumentField types\n",
        "        elif isinstance(field, fof.EmbeddedDocumentField):\n",
        "            if hasattr(field, 'document_type') and field.document_type is not None:\n",
        "                doc_type = field.document_type.__name__\n",
        "                return f\"{field_type}({doc_type})\"\n",
        "            else:\n",
        "                return f\"{field_type}(Unknown)\"\n",
        "\n",
        "        # Check DictField value types\n",
        "        elif isinstance(field, fof.DictField):\n",
        "            if hasattr(field, 'field') and field.field is not None:\n",
        "                value_type = type(field.field).__name__\n",
        "                return f\"{field_type}({value_type})\"\n",
        "            else:\n",
        "                return f\"{field_type}(Unknown)\"\n",
        "\n",
        "        # For other field types, just return the type name\n",
        "        else:\n",
        "            return field_type\n",
        "\n",
        "    def compare_fields(field1, field2):\n",
        "        \"\"\"Compare two fields and return detailed conflict info\"\"\"\n",
        "        info1 = get_detailed_field_info(field1)\n",
        "        info2 = get_detailed_field_info(field2)\n",
        "\n",
        "        if info1 != info2:\n",
        "            return (info1, info2)\n",
        "        return None\n",
        "\n",
        "    # Check all fields\n",
        "    for field_name in all_fields:\n",
        "        field1 = schema1.get(field_name)\n",
        "        field2 = schema2.get(field_name)\n",
        "\n",
        "        conflict = compare_fields(field1, field2)\n",
        "        if conflict:\n",
        "            conflicts[field_name] = conflict\n",
        "\n",
        "    return conflicts\n",
        "\n",
        "def print_detailed_conflicts(conflicts):\n",
        "    \"\"\"Print conflicts with better formatting\"\"\"\n",
        "    if not conflicts:\n",
        "        print(\"No schema conflicts found!\")\n",
        "        return\n",
        "\n",
        "    print(\"Detailed Schema Conflicts:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Group conflicts by type\n",
        "    listfield_conflicts = {}\n",
        "    regular_conflicts = {}\n",
        "    missing_conflicts = {}\n",
        "\n",
        "    for field, (type1, type2) in conflicts.items():\n",
        "        if \"ListField\" in type1 or \"ListField\" in type2:\n",
        "            listfield_conflicts[field] = (type1, type2)\n",
        "        elif \"Missing\" in type1 or \"Missing\" in type2:\n",
        "            missing_conflicts[field] = (type1, type2)\n",
        "        else:\n",
        "            regular_conflicts[field] = (type1, type2)\n",
        "\n",
        "    if listfield_conflicts:\n",
        "        print(\"\\nListField Element Type Conflicts:\")\n",
        "        print(\"-\" * 35)\n",
        "        for field, (type1, type2) in listfield_conflicts.items():\n",
        "            print(f\"  {field}:\")\n",
        "            print(f\"    Dataset1: {type1}\")\n",
        "            print(f\"    Dataset2: {type2}\")\n",
        "\n",
        "    if regular_conflicts:\n",
        "        print(\"\\nRegular Field Type Conflicts:\")\n",
        "        print(\"-\" * 30)\n",
        "        for field, (type1, type2) in regular_conflicts.items():\n",
        "            print(f\"  {field}:\")\n",
        "            print(f\"    Dataset1: {type1}\")\n",
        "            print(f\"    Dataset2: {type2}\")\n",
        "\n",
        "    if missing_conflicts:\n",
        "        print(\"\\nMissing Fields:\")\n",
        "        print(\"-\" * 15)\n",
        "        for field, (type1, type2) in missing_conflicts.items():\n",
        "            print(f\"  {field}:\")\n",
        "            print(f\"    Dataset1: {type1}\")\n",
        "            print(f\"    Dataset2: {type2}\")\n",
        "\n",
        "detailed_conflicts = diagnose_detailed_schema_conflicts(dataset1, dataset2)\n",
        "print_detailed_conflicts(detailed_conflicts)\n",
        "\n",
        "# Also get some sample data to understand the actual content\n",
        "print(\"\\nSample Data Analysis:\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "def analyze_sample_data(dataset, dataset_name, max_samples=3):\n",
        "    \"\"\"Analyze actual data in samples to understand content types\"\"\"\n",
        "    print(f\"\\n{dataset_name} Sample Data:\")\n",
        "\n",
        "    for i, sample in enumerate(dataset.take(max_samples)):\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "\n",
        "        # Check ListField contents\n",
        "        listfield_names = []\n",
        "        for field_name, field in dataset.get_field_schema().items():\n",
        "            if isinstance(field, fof.ListField):\n",
        "                listfield_names.append(field_name)\n",
        "\n",
        "        for field_name in listfield_names[:5]:  # Show first 5 ListFields\n",
        "            if hasattr(sample, field_name):\n",
        "                value = getattr(sample, field_name)\n",
        "                if value is not None and len(value) > 0:\n",
        "                    print(f\"  {field_name}: {value[:3]}... (len={len(value)}, types={[type(x).__name__ for x in value[:3]]})\")\n",
        "\n",
        "analyze_sample_data(dataset1, \"Dataset1\")\n",
        "analyze_sample_data(dataset2, \"Dataset2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnWh2kpa0ch2"
      },
      "source": [
        "# Create a new schema unified schema and convert all entries to respective types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP_21khdjTzX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def safe_resolve_conflicts_fixed(dataset1, dataset2, target_name):\n",
        "    \"\"\"Safely resolve conflicts with proper schema handling\"\"\"\n",
        "\n",
        "    # Delete existing dataset if it exists\n",
        "    try:\n",
        "        existing = fo.load_dataset(target_name)\n",
        "        existing.delete()\n",
        "        print(f\"Deleted existing dataset: {target_name}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Create new unified dataset\n",
        "    unified = fo.Dataset(target_name)\n",
        "\n",
        "    # Define field type mappings\n",
        "    listfield_conversions = {\n",
        "        # Nutritional data should be floats\n",
        "        'kcal_per_plate': float,\n",
        "        'kj_per_plate': float,\n",
        "        'fat_per_plate': float,\n",
        "        'saturated_fat_per_plate': float,\n",
        "        'carbohydrates_per_plate': float,\n",
        "        'sugar_per_plate': float,\n",
        "        'protein_per_plate': float,\n",
        "        'salt_per_plate': float,\n",
        "        'weight_per_portion': float,\n",
        "        'weight_per_plate': float,\n",
        "        'return_quantity': float,\n",
        "        'return_percentage': float,\n",
        "\n",
        "        # Portions should be integers\n",
        "        'number_of_portions': int,\n",
        "\n",
        "        # IDs and names should be strings\n",
        "        'bon_id': str,\n",
        "        'article_number': str,\n",
        "        'ingredient_name': str,\n",
        "        'piece_article': str,\n",
        "    }\n",
        "\n",
        "    single_field_conversions = {\n",
        "        # Before measurements (floats)\n",
        "        'salt_before': float,\n",
        "        'carbohydrates_before': float,\n",
        "        'kcal_before': float,\n",
        "        'saturated_fat_before': float,\n",
        "        'kj_before': float,\n",
        "        'fat_before': float,\n",
        "        'sugar_before': float,\n",
        "        'protein_before': float,\n",
        "\n",
        "        # After measurements (integers)\n",
        "        'saturated_fat_after': int,\n",
        "        'fat_after': int,\n",
        "        'kcal_after': int,\n",
        "        'kj_after': int,\n",
        "        'protein_after': int,\n",
        "        'sugar_after': int,\n",
        "        'salt_after': int,\n",
        "        'carbohydrates_after': int,\n",
        "    }\n",
        "\n",
        "    def convert_value(value, target_type):\n",
        "        \"\"\"Convert a single value to target type, handling None properly\"\"\"\n",
        "        if value is None or value == '' or value == 'None':\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if target_type == float:\n",
        "                return float(value)\n",
        "            elif target_type == int:\n",
        "                return int(float(value))\n",
        "            elif target_type == str:\n",
        "                return str(value)\n",
        "            else:\n",
        "                return value\n",
        "        except (ValueError, TypeError):\n",
        "            return None\n",
        "\n",
        "    def convert_list_values(value_list, target_type):\n",
        "        \"\"\"Convert all values in a list to target type, filtering out None values\"\"\"\n",
        "        if value_list is None:\n",
        "            return None\n",
        "\n",
        "        if not isinstance(value_list, list):\n",
        "            # If it's not a list, try to convert the single value and make it a list\n",
        "            converted = convert_value(value_list, target_type)\n",
        "            return [converted] if converted is not None else []\n",
        "\n",
        "        converted_list = []\n",
        "        for item in value_list:\n",
        "            converted = convert_value(item, target_type)\n",
        "            if converted is not None:  # Only add non-None values\n",
        "                converted_list.append(converted)\n",
        "\n",
        "        # Return None if list is empty, otherwise return the converted list\n",
        "        return converted_list if converted_list else None\n",
        "\n",
        "    def create_clean_sample(original_sample):\n",
        "        \"\"\"Create a new sample with proper type conversions\"\"\"\n",
        "\n",
        "        new_sample = fo.Sample(filepath=original_sample.filepath)\n",
        "\n",
        "        for field_name in original_sample.field_names:\n",
        "            if field_name in ['id', 'filepath']:\n",
        "                continue\n",
        "\n",
        "            value = original_sample[field_name]\n",
        "\n",
        "            # Skip None values entirely to avoid schema inference issues\n",
        "            if value is None:\n",
        "                continue\n",
        "\n",
        "            # Handle ListField conversions\n",
        "            if field_name in listfield_conversions:\n",
        "                target_type = listfield_conversions[field_name]\n",
        "                converted_value = convert_list_values(value, target_type)\n",
        "\n",
        "                # Only set the field if we have valid data\n",
        "                if converted_value is not None and len(converted_value) > 0:\n",
        "                    new_sample[field_name] = converted_value\n",
        "\n",
        "            # Handle single field conversions\n",
        "            elif field_name in single_field_conversions:\n",
        "                target_type = single_field_conversions[field_name]\n",
        "                converted_value = convert_value(value, target_type)\n",
        "\n",
        "                # Only set the field if we have valid data\n",
        "                if converted_value is not None:\n",
        "                    new_sample[field_name] = converted_value\n",
        "\n",
        "            # Handle other fields (copy as-is if not None)\n",
        "            else:\n",
        "                try:\n",
        "                    new_sample[field_name] = value\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not set field {field_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        return new_sample\n",
        "\n",
        "    # Process Dataset2 first (it has cleaner schema)\n",
        "    print(\"Processing Dataset2 samples first...\")\n",
        "    dataset2_samples = []\n",
        "    for sample in dataset2.iter_samples(progress=True):\n",
        "        try:\n",
        "            clean_sample = create_clean_sample(sample)\n",
        "            dataset2_samples.append(clean_sample)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing Dataset2 sample {sample.id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Add Dataset2 samples first to establish schema\n",
        "    print(\"Adding Dataset2 samples to establish schema...\")\n",
        "    if dataset2_samples:\n",
        "        unified.add_samples(dataset2_samples[:10])  # Add first 10 to establish schema\n",
        "\n",
        "        # Add remaining Dataset2 samples\n",
        "        if len(dataset2_samples) > 10:\n",
        "            remaining_samples = dataset2_samples[10:]\n",
        "            batch_size = 100\n",
        "            for i in range(0, len(remaining_samples), batch_size):\n",
        "                batch = remaining_samples[i:i + batch_size]\n",
        "                unified.add_samples(batch)\n",
        "\n",
        "    # Now process Dataset1 samples\n",
        "    print(\"Processing Dataset1 samples...\")\n",
        "    dataset1_samples = []\n",
        "    for sample in dataset1.iter_samples(progress=True):\n",
        "        try:\n",
        "            clean_sample = create_clean_sample(sample)\n",
        "            dataset1_samples.append(clean_sample)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing Dataset1 sample {sample.id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Add Dataset1 samples in batches\n",
        "    print(\"Adding Dataset1 samples...\")\n",
        "    if dataset1_samples:\n",
        "        batch_size = 100\n",
        "        for i in range(0, len(dataset1_samples), batch_size):\n",
        "            batch = dataset1_samples[i:i + batch_size]\n",
        "            try:\n",
        "                unified.add_samples(batch)\n",
        "            except Exception as e:\n",
        "                print(f\"Error adding batch {i//batch_size + 1}: {e}\")\n",
        "                # Try adding samples one by one for this batch\n",
        "                for j, sample in enumerate(batch):\n",
        "                    try:\n",
        "                        unified.add_sample(sample)\n",
        "                    except Exception as sample_error:\n",
        "                        print(f\"  Failed to add sample {i+j}: {sample_error}\")\n",
        "                        continue\n",
        "\n",
        "    unified.persistent = True\n",
        "\n",
        "    print(f\"Successfully created unified dataset with {len(unified)} samples\")\n",
        "\n",
        "    # Print final schema for verification\n",
        "    print(\"\\nFinal schema for key fields:\")\n",
        "    schema = unified.get_field_schema()\n",
        "    key_fields = ['kcal_per_plate', 'kj_per_plate', 'kcal_before', 'kcal_after']\n",
        "    for field_name in key_fields:\n",
        "        if field_name in schema:\n",
        "            field_type = type(schema[field_name]).__name__\n",
        "            print(f\"  {field_name}: {field_type}\")\n",
        "\n",
        "    return unified\n",
        "\n",
        "# Usage\n",
        "unified_dataset = safe_resolve_conflicts_fixed(dataset1, dataset2, \"food_waste_final_v11\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbqcratopban"
      },
      "outputs": [],
      "source": [
        "unified_ds = fo.load_dataset(\"food_waste_final_v11\")\n",
        "print(f\"All entries are merged: {len(unified_ds) == len(dataset2)+len(dataset1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e57qjF2gqF_a"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(unified_ds)\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRLIn8XvgVqi"
      },
      "source": [
        "# Save dataset in drive externally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5ZJ4ZD7iudS"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Export your dataset to Drive\n",
        "export_path = \"/content/drive/MyDrive/food_waste_merged_v11\"\n",
        "\n",
        "# Export as FiftyOne dataset (preserves all metadata, embeddings, etc.)\n",
        "unified_dataset.export(\n",
        "    export_dir=export_path,\n",
        "    dataset_type=fo.types.FiftyOneDataset\n",
        ")\n",
        "\n",
        "print(f\"Dataset exported to: {export_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf7r6DAGxxDC"
      },
      "outputs": [],
      "source": [
        "unified_ds"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
