{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Library Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install fiftyone clip sentence-transformers > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Library Imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PENhBUNx2M6-",
        "outputId": "530aa14f-4eab-454a-c161-008609389cdf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import fiftyone as fo\n",
        "import numpy as np\n",
        "import clip\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch\n",
        "from scipy.stats import spearmanr\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torch.optim as optim\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa4MGJgH2VaK",
        "outputId": "a37cd8e3-2a36-4d79-d4bc-38b4917f23d3"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/all_data\"\n",
        "loaded_dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=dataset_path, dataset_type=fo.types.FiftyOneDataset, name=\"all_data\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvsg8vIk2vnb",
        "outputId": "3edfcbb8-cbb0-4e15-c337-3c488722a5a3"
      },
      "outputs": [],
      "source": [
        "len(loaded_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transform the Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLZYLzbGWG-e",
        "outputId": "1398b848-10d2-419e-8d9b-40a6c507ce4e"
      },
      "outputs": [],
      "source": [
        "# Complete ingredient translation and cleanup\n",
        "def clean_ingredient_name(name):\n",
        "    \"\"\"Remove whitespace and normalize formatting\"\"\"\n",
        "    cleaned = name.strip().lower()\n",
        "    # Remove trailing parentheses artifacts\n",
        "    if cleaned.endswith(\")\"):\n",
        "        cleaned = cleaned.rstrip(\")\")\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "# German to English translation dictionary\n",
        "ingredient_mapping = {\n",
        "    # Basic ingredients\n",
        "    \"apfelessig\": \"apple-vinegar\",\n",
        "    \"apfelmus\": \"applesauce\",\n",
        "    \"blumenkohl\": \"cauliflower\",\n",
        "    \"brokkoli\": \"broccoli\",\n",
        "    \"dill\": \"dill\",\n",
        "    \"erbsen\": \"peas\",\n",
        "    \"eisbergsalat\": \"iceberg-lettuce\",\n",
        "    \"kartoffeln\": \"potatoes\",\n",
        "    \"kartoffelwürfel\": \"diced-potatoes\",\n",
        "    \"kartoffelpüree\": \"mashed-potatoes\",\n",
        "    \"kohlrabi\": \"kohlrabi\",\n",
        "    \"möhre\": \"carrot\",\n",
        "    \"nudeln\": \"noodles\",\n",
        "    \"paprika\": \"bell-pepper\",\n",
        "    \"pilze\": \"mushrooms\",\n",
        "    \"reis\": \"rice\",\n",
        "    \"rosenkohl\": \"brussels-sprouts\",\n",
        "    \"rotkohl\": \"red-cabbage\",\n",
        "    \"spinat\": \"spinach\",\n",
        "    \"zucchini\": \"zucchini\",\n",
        "    \"zwiebel\": \"onion\",\n",
        "    \"sultaninen\": \"raisins\",\n",
        "    \"grüne bohnen\": \"green-beans\",\n",
        "    \"grünkohl\": \"kale\",\n",
        "    \"wirsing\": \"savoy-cabbage\",\n",
        "    \"sauerkraut\": \"sauerkraut\",\n",
        "    \"krautsalat\": \"coleslaw\",\n",
        "    \"cherrytomate halbiert\": \"cherry-tomatoes-halved\",\n",
        "    \"gekochtes ei\": \"boiled-egg\",\n",
        "    # Meat and fish\n",
        "    \"hähnchen\": \"chicken\",\n",
        "    \"hähnchenschnitzel\": \"chicken-schnitzel\",\n",
        "    \"hähnchenstreifen\": \"chicken-strips\",\n",
        "    \"rinderbraten\": \"roast-beef\",\n",
        "    \"rindergulasch\": \"beef-goulash\",\n",
        "    \"rinderroulade\": \"beef-roulade\",\n",
        "    \"schweinelachssteak\": \"pork-loin-steak\",\n",
        "    \"schweinenackenbraten\": \"pork-neck-roast\",\n",
        "    \"kasseler\": \"smoked-pork-chop\",\n",
        "    \"frikadelle\": \"meatball\",\n",
        "    \"hackbraten\": \"meatloaf\",\n",
        "    \"seelachs\": \"pollock\",\n",
        "    \"buntbarsch\": \"perch\",\n",
        "    \"heringsstipp\": \"herring-salad\",\n",
        "    # Sausages and processed meat\n",
        "    \"schinken mettwurst\": \"ham-sausage\",\n",
        "    \"speckwürfel\": \"bacon-cubes\",\n",
        "    # Sauces and condiments\n",
        "    \"bechamel\": \"bechamel-sauce\",\n",
        "    \"bratenjus\": \"gravy\",\n",
        "    \"braune sauce\": \"brown-sauce\",\n",
        "    \"currysauce\": \"curry-sauce\",\n",
        "    \"helle sauce\": \"light-sauce\",\n",
        "    \"malzbier-senf-sauce\": \"malt-beer-mustard-sauce\",\n",
        "    \"dunkle balsamico-sauce\": \"dark-balsamic-sauce\",\n",
        "    \"tomaten-curry-sauce\": \"tomato-curry-sauce\",\n",
        "    \"tomatensauce\": \"tomato-sauce\",\n",
        "    \"vanillesauce\": \"vanilla-sauce\",\n",
        "    \"zitronensauce\": \"lemon-sauce\",\n",
        "    \"geräucherte paprikasauce\": \"smoked-paprika-sauce\",\n",
        "    \"senf\": \"mustard\",\n",
        "    \"dressing portion\": \"dressing-portion\",\n",
        "    # Dairy and cream\n",
        "    \"sahne\": \"cream\",\n",
        "    \"pflanzencreme\": \"plant-based-cream\",\n",
        "    \"gouda gerieben\": \"grated-gouda\",\n",
        "    # Grains and starches\n",
        "    \"haferbrei\": \"oatmeal\",\n",
        "    \"linsen\": \"lentils\",\n",
        "    \"eierspätzle\": \"egg-spaetzle\",\n",
        "    \"schupfnudeln\": \"potato-noodles\",\n",
        "    \"semmelknödel\": \"bread-dumplings\",\n",
        "    \"dampfnudel\": \"steamed-dumpling\",\n",
        "    \"reibekuchen\": \"potato-pancakes\",\n",
        "    # Spices\n",
        "    \"kümmel gemahlen\": \"ground-caraway\",\n",
        "    # Handle duplicates that already exist in English\n",
        "    \"goulash\": \"beef-goulash\",\n",
        "    \"chickpeas\": \"chickpeas\",\n",
        "    \"lentil-stew\": \"lentil-stew\",\n",
        "}\n",
        "\n",
        "\n",
        "def create_cleaned_ingredient_mapping(raw_ingredients):\n",
        "    \"\"\"\n",
        "    Create mapping from original ingredient names to cleaned/translated versions\n",
        "    \"\"\"\n",
        "    ingredient_cleanup_map = {}\n",
        "\n",
        "    for ing in raw_ingredients:\n",
        "        cleaned = clean_ingredient_name(ing)\n",
        "\n",
        "        # Apply translation if exists\n",
        "        if cleaned in ingredient_mapping:\n",
        "            final_name = ingredient_mapping[cleaned]\n",
        "        else:\n",
        "            # Keep English names as-is, just cleaned\n",
        "            final_name = cleaned.replace(\n",
        "                \" \", \"-\"\n",
        "            )  # Convert spaces to hyphens for consistency\n",
        "\n",
        "        ingredient_cleanup_map[ing] = final_name\n",
        "\n",
        "    return ingredient_cleanup_map\n",
        "\n",
        "\n",
        "# Usage in your code:\n",
        "raw_ingredients = {ing for s in loaded_dataset for ing in s[\"ingredient_name\"]}\n",
        "ingredient_cleanup_map = create_cleaned_ingredient_mapping(raw_ingredients)\n",
        "\n",
        "# Get unique cleaned ingredients\n",
        "cleaned_ingredients = set(ingredient_cleanup_map.values())\n",
        "all_ing = sorted(cleaned_ingredients)\n",
        "ing2idx = {ing: i for i, ing in enumerate(all_ing)}\n",
        "idx2ing = {i: ing for ing, i in ing2idx.items()}\n",
        "\n",
        "print(f\"Reduced from {len(raw_ingredients)} to {len(all_ing)} unique ingredients\")\n",
        "print(\"\\nSample mappings:\")\n",
        "for orig, cleaned in list(ingredient_cleanup_map.items())[:10]:\n",
        "    print(f\"'{orig}' -> '{cleaned}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "_fGAxhy2Xd3K",
        "outputId": "2657a3ba-4332-4a3b-8260-8bb28b0b5a44"
      },
      "outputs": [],
      "source": [
        "# # Clone the original dataset\n",
        "# cleaned_dataset = loaded_dataset.clone(\"food_waste_cleaned\")\n",
        "\n",
        "# # Apply transformations to the cloned dataset\n",
        "# for sample in cleaned_dataset:\n",
        "#     original_ingredients = sample[\"ingredient_name\"]\n",
        "#     cleaned_ingredients = [ingredient_cleanup_map[ing] for ing in original_ingredients]\n",
        "#     sample[\"ingredient_name\"] = cleaned_ingredients\n",
        "#     sample.save()\n",
        "\n",
        "# # Use the cleaned dataset for training\n",
        "# loaded_dataset = cleaned_dataset\n",
        "\n",
        "# # Export your dataset to Drive\n",
        "# export_path = \"/content/drive/MyDrive/cleaned_dataset_final\"\n",
        "\n",
        "# # Export as FiftyOne dataset (preserves all metadata, embeddings, etc.)\n",
        "# cleaned_dataset.export(\n",
        "#     export_dir=export_path,\n",
        "#     dataset_type=fo.types.FiftyOneDataset\n",
        "# )\n",
        "\n",
        "# print(f\"Dataset exported to: {export_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the clean data for training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8sopiiyaK92",
        "outputId": "c3176afc-4ba9-4b8b-b2a8-7dde6ad246dc"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/cleaned_dataset_final\"\n",
        "loaded_dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=dataset_path,\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    name=\"cleaned_dataset_final\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31TVRojdXmZO",
        "outputId": "c6778453-9cba-49f8-b1e0-287ccfa99a36"
      },
      "outputs": [],
      "source": [
        "# Load CLIP model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Get all ingredients (same as before)\n",
        "all_ing = sorted({ing for s in loaded_dataset for ing in s[\"ingredient_name\"]})\n",
        "ing2idx = {ing: i for i, ing in enumerate(all_ing)}\n",
        "idx2ing = {i: ing for ing, i in ing2idx.items()}\n",
        "\n",
        "# Create CLIP text embeddings\n",
        "print(\"Encoding ingredients with CLIP...\")\n",
        "ingredient_texts = [\n",
        "    f\"a photo of {ing}\" for ing in all_ing\n",
        "]  # Add context for better embeddings\n",
        "\n",
        "# Encode in batches to avoid memory issues\n",
        "batch_size = 64\n",
        "embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(ingredient_texts), batch_size):\n",
        "        batch_texts = ingredient_texts[i : i + batch_size]\n",
        "        tokenized = clip.tokenize(batch_texts).to(device)\n",
        "        batch_embeddings = clip_model.encode_text(tokenized)\n",
        "        embeddings.append(batch_embeddings.float().cpu())\n",
        "\n",
        "emb_matrix = torch.cat(embeddings, dim=0).numpy()\n",
        "NUM_ING = len(ing2idx)\n",
        "print(f\"Created embeddings shape: {emb_matrix.shape}\")\n",
        "print(ing2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D042geVZ_pTT"
      },
      "outputs": [],
      "source": [
        "class FoodWasteTorchDataset(Dataset):\n",
        "    def __init__(self, view, img_tfms):\n",
        "        self.view = view\n",
        "        self.ids = view.values(\"id\")\n",
        "        self.img_tfms = img_tfms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.view[self.ids[idx]]\n",
        "        img = self.img_tfms(Image.open(s.filepath).convert(\"RGB\"))\n",
        "\n",
        "        # ingredient embeddings (variable-length)\n",
        "        emb = torch.from_numpy(\n",
        "            np.stack([emb_matrix[ing2idx[i]] for i in s[\"ingredient_name\"]])\n",
        "        )\n",
        "\n",
        "        # regression target (g of waste per ingredient)\n",
        "        tgt = torch.zeros(len(ing2idx), dtype=torch.float32)\n",
        "        for ing, amt in zip(s[\"ingredient_name\"], s[\"return_quantity\"]):\n",
        "            if amt is not None:\n",
        "                tgt[ing2idx[ing]] = amt\n",
        "        return img, emb, tgt\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    imgs, embs, tgts = zip(*batch)\n",
        "    return torch.stack(imgs), list(embs), torch.stack(tgts)\n",
        "\n",
        "\n",
        "tfms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_view = loaded_dataset.match({\"split\": \"train\"})\n",
        "test_view = loaded_dataset.match({\"split\": \"test\"})\n",
        "train_dataset = FoodWasteTorchDataset(train_view, tfms)\n",
        "test_dataset = FoodWasteTorchDataset(test_view, tfms)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_iugzkx_5fd"
      },
      "outputs": [],
      "source": [
        "def jitter_targets(targets, epsilon=1.0):\n",
        "    mask = targets > 0\n",
        "    noise = torch.empty_like(targets).uniform_(-epsilon, epsilon)\n",
        "    jittered = targets + noise * mask.float()\n",
        "    return torch.clamp(jittered, min=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwYNJBovAB7A",
        "outputId": "0e4fb3d5-4edc-4d2e-8e65-3d702c7bcae0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "freq = torch.zeros(NUM_ING, dtype=torch.long)\n",
        "\n",
        "for sample in train_dataset:\n",
        "    freq += sample[2] > 0\n",
        "\n",
        "w = 1.0 / torch.log(freq.float() + 2)\n",
        "w = w / w.mean()\n",
        "\n",
        "weight_vec = w.to(device)\n",
        "print(\"max weight:\", weight_vec.max().item(), \"min weight:\", weight_vec.min().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjrCvu6HAC-5",
        "outputId": "75efe1a6-2e8b-4200-b178-6b3d109611c9"
      },
      "outputs": [],
      "source": [
        "class FoodWastePredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Baseline: ResNet image encoder + pooled SBERT ingredient encoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *, embedding_dim: int = 512, num_ingredients: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # ---------- image branch ----------\n",
        "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        img_feats = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(img_feats, 128)  # → (B, 128)\n",
        "\n",
        "        # ---------- ingredient branch ----------\n",
        "        self.ing_mlp = nn.Sequential(  # (B, 512) → (B, 128)\n",
        "            nn.Linear(embedding_dim, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # ---------- fusion & head ----------\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(128 + 128, 128), nn.ReLU(inplace=True), nn.Dropout(0.2)\n",
        "        )\n",
        "        self.q_proj = nn.Linear(128, embedding_dim, bias=False)\n",
        "        self.regressor = nn.Linear(128, num_ingredients)  # (B,N_ing)\n",
        "\n",
        "    def forward(self, images, artikel_emb, return_total=True):\n",
        "        \"\"\"\n",
        "        images: Tensor [B, 3, H, W]\n",
        "        artikel_emb: Tensor [B, L, 512] or list of [L_i, 512] tensors\n",
        "        \"\"\"\n",
        "        # --- image path ---\n",
        "        img_feat = self.resnet(images)  # (B,128)\n",
        "\n",
        "        # ----- text path -----\n",
        "        if isinstance(artikel_emb, list):  # list → pad\n",
        "            artikel_emb = nn.utils.rnn.pad_sequence(artikel_emb, batch_first=True)\n",
        "        # ing_feat = self.ing_mlp(artikel_emb.mean(dim=1))    # (B,128)\n",
        "        token_feat = self.ing_mlp(artikel_emb)  # (B,L,128)\n",
        "        query = self.q_proj(img_feat).unsqueeze(2)\n",
        "        attn_logits = (artikel_emb @ query).squeeze(2)\n",
        "        attn_weights = torch.softmax(attn_logits, dim=1)\n",
        "        ing_feat = (attn_weights.unsqueeze(2) * token_feat).sum(dim=1)  # (B,128)\n",
        "        # ----- fuse & slot predictions -----\n",
        "        combined = torch.cat((img_feat, ing_feat), dim=1)\n",
        "        fused = self.fusion(combined)  # (B, 128)\n",
        "        return F.relu(self.regressor(fused))\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "embedding_dim = 512  # SBERT output size\n",
        "model = FoodWastePredictor(embedding_dim=embedding_dim, num_ingredients=NUM_ING)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82jl1jqmBIkg",
        "outputId": "bb479b90-6824-4174-f199-5a1acb66892c"
      },
      "outputs": [],
      "source": [
        "# Define Loss Function (Mean Squared Error for regression)\n",
        "# With reduction='none' returns the element-wise squared error\n",
        "criterion = nn.L1Loss(reduction=\"none\")\n",
        "# Define Optimizer (Adam optimizer)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 30\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, ingredients, targets in train_loader:\n",
        "        images = images.to(device)\n",
        "        ingredients = [seq.to(device) for seq in ingredients]\n",
        "        targets = targets.to(device)\n",
        "        targets = jitter_targets(targets, epsilon=1.0)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, ingredients)\n",
        "        tot_pred = outputs.sum(1)\n",
        "        tot_true = targets.sum(1)\n",
        "        diff_sq = criterion(outputs, targets)\n",
        "        loss = (diff_sq * weight_vec).mean()\n",
        "        total_loss = loss + 0.001 * F.l1_loss(tot_pred, tot_true)\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXh-Dpt0GhIV"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, ingredients, targets in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            ingredients = [ing.to(device) for ing in ingredients]\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            preds = model(imgs, ingredients)\n",
        "\n",
        "            y_true.append(targets.cpu().numpy())\n",
        "            y_pred.append(preds.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(y_true, axis=0)  # (N, num_ing)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "    true_total = y_true.sum(axis=1)\n",
        "    pred_total = y_pred.sum(axis=1)\n",
        "    return y_true, y_pred, true_total, pred_total\n",
        "\n",
        "\n",
        "def evaluate(y_true, y_pred, true_total, pred_total):\n",
        "    mae_total = np.abs(pred_total - true_total).mean()\n",
        "    rho, _ = spearmanr(true_total, pred_total)\n",
        "\n",
        "    print(f\"\\nTotal-waste MAE: {mae_total:.3f}   Spearman ρ: {rho:.3f}\")\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R²\": r2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYzo6WCzCL2D"
      },
      "outputs": [],
      "source": [
        "y_true, y_pred, true_total, pred_total = get_predictions(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_ZOAjAQGYZK"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, ingredients, targets in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            ingredients = [ing.to(device) for ing in ingredients]\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            preds = model(imgs, ingredients)\n",
        "\n",
        "            y_true.append(targets.cpu().numpy())\n",
        "            y_pred.append(preds.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(y_true, axis=0)  # (N, num_ing)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "    true_total = y_true.sum(axis=1)\n",
        "    pred_total = y_pred.sum(axis=1)\n",
        "    return y_true, y_pred, true_total, pred_total\n",
        "\n",
        "\n",
        "def evaluate(y_true, y_pred, true_total, pred_total):\n",
        "    mae_total = np.abs(pred_total - true_total).mean()\n",
        "    rho, _ = spearmanr(true_total, pred_total)\n",
        "\n",
        "    print(f\"\\nTotal-waste MAE: {mae_total:.3f}   Spearman ρ: {rho:.3f}\")\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R²\": r2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5BZ1agmG3ms"
      },
      "outputs": [],
      "source": [
        "y_true, y_pred, true_total, pred_total = get_predictions(model, test_loader, device)\n",
        "metrics = evaluate(y_true, y_pred, true_total, pred_total)\n",
        "print(\"\\nTest metrics:\")\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k:>4}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx2G5moSG5TI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(true_total, pred_total, alpha=0.4)\n",
        "plt.xlabel(\"True total waste\")\n",
        "plt.ylabel(\"Predicted total\")\n",
        "plt.xlim(0)\n",
        "plt.ylim(0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ2tEFzsHHKt"
      },
      "outputs": [],
      "source": [
        "bias = (y_pred - y_true).mean(axis=0)  # shape (num_ing,)\n",
        "worst = np.argsort(np.abs(bias))[-10:]\n",
        "for i in worst[::-1]:\n",
        "    print(f\"{idx2ing[i]:<30} bias={bias[i]:+.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKgFhG3KHq1Q",
        "outputId": "e38999ec-965c-4dbd-eec3-45d21d28ba0a"
      },
      "outputs": [],
      "source": [
        "bias = (y_pred - y_true).mean(axis=0)  # shape (num_ing,)\n",
        "worst = np.argsort(np.abs(bias))[-10:]\n",
        "for i in worst[::-1]:\n",
        "    print(f\"{idx2ing[i]:<30} bias={bias[i]:+.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_predictions_sample_by_sample(model, test_view, device, dataset, transforms, emb_matrix, ing2idx, idx2ing):\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for sample_id in test_view.values(\"id\"):\n",
        "            # Get the sample from test_view\n",
        "            fo_sample = test_view[sample_id]\n",
        "            \n",
        "            # Apply transforms (use your existing transforms)\n",
        "            img = transforms(Image.open(fo_sample.filepath).convert(\"RGB\")).unsqueeze(0)\n",
        "            \n",
        "            # Get ingredients for this sample\n",
        "            cleaned_ingredients = [ing for ing in fo_sample.ingredient_name]\n",
        "            ingredient_emb = torch.from_numpy(\n",
        "                np.stack([emb_matrix[ing2idx[i]] for i in cleaned_ingredients])\n",
        "            ).float().to(device)  # Make sure it's float32\n",
        "            \n",
        "            img = img.to(device)\n",
        "            \n",
        "            # Get prediction\n",
        "            pred = model(img, [ingredient_emb])  # Pass as list\n",
        "            pred_np = pred.squeeze(0).cpu().numpy()  # Remove batch dimension\n",
        "            \n",
        "            # Upload to FiftyOne dataset (this is the key part!)\n",
        "            sample = dataset[sample_id]  # Get from FiftyOne dataset\n",
        "            non_zero_indices = np.where(pred_np > 0)[0]\n",
        "            \n",
        "            if len(non_zero_indices) > 0:\n",
        "                predicted_ingredients = [idx2ing[idx] for idx in non_zero_indices]\n",
        "                predicted_values = pred_np[non_zero_indices].tolist()\n",
        "                \n",
        "                sample['predicted_ingredients'] = predicted_ingredients\n",
        "                sample['predicted_amounts'] = predicted_values\n",
        "                sample['total_predicted_waste'] = float(pred_np.sum())\n",
        "            else:\n",
        "                sample['predicted_ingredients'] = []\n",
        "                sample['predicted_amounts'] = []\n",
        "                sample['total_predicted_waste'] = 0.0\n",
        "                \n",
        "            sample.save()  # This saves to FiftyOne!\n",
        "            \n",
        "            print(f\"Processed sample {sample_id}\")\n",
        "\n",
        "# Usage:\n",
        "upload_predictions_sample_by_sample(\n",
        "    model, train_view, device, loaded_dataset, \n",
        "    tfms, emb_matrix, ing2idx, idx2ing\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
